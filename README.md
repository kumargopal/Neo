# ðŸ§  Neo

A **localâ€‘first framework** for building, testing, and running *neuros*â€”small, hotâ€‘swappable Python skills that a planning LLM stitches into taskâ€‘flows (directed acyclic graphs / DAGs).

Neo lets you:

* Chat naturally while the planner decides which neuros to invoke.
* Swap neuros at runtime without restarting the server (hotâ€‘reload).
* Drop into developerâ€‘mode to scaffold, edit, diff and save neuros from the same chat window.

---

## TableÂ ofÂ Contents

1. [Architectureâ€¯Overview](#architecture-overview)
2. [CoreÂ PackageÂ Internals](#core-package-internals)
3. [QuickÂ Start](#quick-start)
4. [ProfilesÂ &Â Modes](#profiles--modes)
5. [Developerâ€‘ModeÂ Workflow](#developer-mode-workflow)
6. [Codeâ€‘ProjectÂ Workflow](#code-project-workflow)
7. [ExampleÂ Conversations](#example-conversations)
8. [TestingÂ withÂ Scenarios](#testing-with-scenarios)
9. [Contributing](#contributing)
10. [License](#license)

---

## ArchitectureÂ Overview

| Layer / Component  | Purpose                                                                                                                        |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------ |
| **FastAPI server** | Exposes `POST /chat` and `WS /ws/{cid}` so UIs and scripts can stream messages & node events in realâ€‘time.                     |
| **CLI client**     | A Richâ€‘TTY frontâ€‘end featuring coloured message panels, live DAG tracing, and power commands (`/dev on`, `/dag show`, â€¦).      |
| **Brain**          | Maintains perâ€‘conversation state, chooses a *profile* (plannerâ€¯+â€¯replier combo) and launches the **Executor**.                 |
| **Executor**       | Walks a DAG, runs each neuro in sequence, gathers stdout, and emits `node.*` + `task.done` events back via WebSocket.          |
| **NeuroÂ factory**  | Hotâ€‘reloads every `neuros/*/conf.json`Â +Â `code.py`; injects a readyâ€‘toâ€‘use `BaseBrain` instance and `prompt.txt` into `state`. |
| **Profiles**       | Thin JSON presetsâ€”*general*, *code\_dev*, *neuro\_dev*â€”that switch planners, repliers and visible neuros on the fly.           |

<p align="center"><em>All components live under <code>core/</code> or at projectâ€‘root.<br>Everything is pure Python 3.11+ with zero external services required (apart from your OpenAI key).</em></p>

---

## CoreÂ PackageÂ Internals

| Module                       | How it works                                                                                                                                                                                                                                                                                                                                                                   |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **`core/brain.py`**          | Orchestrates a chat. It: (1) logs turns into `Conversation`; (2) runs **`intent_classifier`**; (3) picks a profile & planner; (4) publishes debug + node events over the pubâ€‘sub **hub**.                                                                                                                                                                                      |
| **`core/executor.py`**       | Receives a **flow** object (`{"start":"n0","nodes":{â€¦}}`). For each node it:<br>1. emits `node.start`<br>2. `await factory.run(neuro, state, **params)`<br>3. merges outputs into `state`<br>4. emits `node.done` (or error) and, if `reply` exists, an `assistant` event.<br>Reâ€‘planning is automatic if a neuro sets `replan=True`.                                          |
| **`core/neuro_factory.py`**  | Scans `neuros/*/conf.json`. Each folder must contain:<br>â€¢ `conf.json`Â (manifest)<br>â€¢ `code.py`Â (async `run`)<br>â€¢ *optional*Â `prompt.txt`.<br>It compiles the code with `exec`, injects `state["__llm"]` (`BaseBrain`), and stores a `BaseNeuro` wrapper in a registry.<br>Runs an async task that watches for fileâ€‘mtime changes every secondâ€”editÂ &Â save â†’ instant reload. |
| **`core/base_brain.py`**     | Thin wrapper around the OpenAI client. Provides `generate_text`, `generate_json`, and a higherâ€‘level `plan()` helper that autoâ€‘parses JSON.                                                                                                                                                                                                                                    |
| **`core/base_neuro.py`**     | A tiny struct holdingÂ `name`, `fn`, `inputs`, `outputs`, `desc`. The factory instantiates this and the executor ultimately calls `.run()`.                                                                                                                                                                                                                                     |
| **`core/conversation.py`**   | Persists each chat in `conversations/<cid>.json`. Provides `.add()` and `.history(n)` helpers so neuros & profiles can access full or sliced history.                                                                                                                                                                                                                          |
| **`core/pubsub.py`**         | Simple asyncio broadcast hub. `hub.queue(cid)` returns an `asyncio.Queue` that the server pushes events to and WebSocket clients consume from.                                                                                                                                                                                                                                 |
| **`core/â€¦/profiles/*.json`** | Each file chooses a planner, a replier and a glob list of visible neuros. Switching profile at runtime simply updates these choices for that conversation.                                                                                                                                                                                                                     |

> **Headsâ€‘upÂ for contributors:** Hotâ€‘reload for `core/` code itself is handled by `uvicorn --reload`; for neuros the factory takes care of reâ€‘exec on change.

---

## QuickÂ Start

```bash
# 1Â Â Install deps (PythonÂ 3.11+)
pip install -r requirements.txt

# 2Â Â Launch the server (reload on code changes)
python server.py

# 3Â Â Open another terminal and chat
python cli_client.py --host localhost --port 8000
```

The CLI greets you withÂ 

```text
neuro ClientÂ v1.0Â Â (cidÂ be3a9d1f)
Type /help for commands
>
```

Type messages as you would in ChatGPT. Toggle goodies:

* `/dag show` â€“ popup a live NetworkX view of the current flow.
* `/dev on`Â /Â `/dev off` â€“ enable or leave developerâ€‘mode.
* `/profile <name>` â€“ jump straight to `general`, `code_dev`, or `neuro_dev`.

---

## ProfilesÂ &Â Modes

| Profile     | Planner        | Replier      | Visible neuros (glob)                  |
| ----------- | -------------- | ------------ | -------------------------------------- |
| `general`   | `planner`      | `reply`      | `*`                                    |
| `code_dev`  | `code_planner` | `code_reply` | `code_*`, `neuro_list`                 |
| `neuro_dev` | `dev_planner`  | `dev_reply`  | `dev_*`, `load_neuro`, `neuro_list`, â€¦ |

Switch at runtime via `/profile code_dev` or the shortcut `/dev on` (`neuro_dev`) and `/dev off` (back to `general`).

---

## Developerâ€‘ModeÂ Workflow

```text
/dev on                       # â†’ neuro_dev profile (dev_planner + dev_reply)
dev_new                       # scaffold a completely new neuro via LLM
dev_edit "add error handling" # patch drafts in memory
dev_show                      # show current drafts (Markdown diff)
dev_save                      # write drafts to neuros/<name>/ (factory reloads)
```

### Hotâ€‘reload timeline

1. **`dev_save`** writes files to `neuros/...`.
2. The **neuroÂ factory** notices `mtime` changed â†’ reâ€‘executes `code.py`.
3. Your new skill is live for the *next* planner runâ€”no restarts.

---

## Codeâ€‘ProjectÂ Workflow

| Command / Intent                      | Typical Flow                                                                                    |
| ------------------------------------- | ----------------------------------------------------------------------------------------------- |
| â€œCreate project `blog`â€               | `code_planner` â†’ `code_project_manager(project_name="blog")`                                    |
| â€œAdd a file `README.md` with headingâ€ | planner adds a `code_file_write` node after ensuring project exists.                            |
| â€œShow filesâ€                          | shortâ€‘form â†’ `code_file_list(glob="**/*")`                                                      |
| â€œOpen projectâ€                        | `code_project_manager(project_name=â€¦)` just switches the cached `__project_root`.               |
| â€œPatch `main.py` to log errorsâ€       | `code_file_read` â†’ `code_file_diff` (diff preview) â†’ await **apply patch** â†’ `code_file_write`. |

`code_dev` profile routes all such utterances automatically; you can still sprinkle `/dev on` if you want to author new neuros midâ€‘conversation.

---

## ExampleÂ Conversations

### 1Â Â GenericÂ Chat (general profile)

```text
user  â€º Hi Neo!
assistant â€º Hello! How can I assist you today?
user  â€º What is the difference between a thread and a process?
assistant â€º â€¦ concise technical explanation â€¦
```

The planner emits `{type:"reply"}`; no neuros run.

### 2Â Â Neuroâ€‘dev Session

```text
user  â€º /dev on
assistant â€º neuroâ€‘dev profile enabled.

user  â€º Create a neuro that locks an Ubuntu screen.
assistant â€º ðŸ†• Draft for **screen_lock_ubuntu** created.
           (shows neuro.json, prompt.txt, code.py)

user  â€º Add error handling and return a boolean success flag.
assistant â€º Shows unified diff preview.

user  â€º apply patch
assistant â€º âœ… Applied patch.

user  â€º dev_save
assistant â€º âœ… Saved 3 file(s).
```

Factory reloads â†’ `screen_lock_ubuntu` is now available.

### 3Â Â Using the New Neuro Immediately

```text
user  â€º Lock my screen please.
assistant â€º ðŸ”’ Screen locked successfully.
```

Planner chose `{name:"screen_lock_ubuntu"}`.

### 4Â Â Codeâ€‘dev Project + Multiâ€‘Neuro orchestration

```text
user  â€º /profile code_dev   # or just stay in dev and switch off later
assistant â€º Codeâ€‘dev profile enabled.

user  â€º Create project log_analyser.
assistant â€º âœ… Project **log_analyser** ready at â€¦

user  â€º Write file log_analyser/main.py that prints "hello".
assistant â€º ðŸ“ Wrote `log_analyser/main.py` (3Â lines).

user  â€º Show project files.
assistant â€º Files:
           â€“ log_analyser/main.py

user  â€º Run the program.            # (not yet implemented)
assistant â€º Currently unsupportedâ€”try adding a devÂ neuro to execute Python.
```

You could now `/dev on` again, `dev_new run_python_file`, implement it, `dev_save`, and immediately do:

```text
user â€º run_python_file path="log_analyser/main.py"
assistant â€º stdout: hello
```

### 5Â Â Composing Neuros Together

```text
user  â€º /dev on
assistant â€º neuroâ€‘dev profile enabled.

user  â€º Create a neuro summarise_logs that reads all *.log files in the active project and returns a markdown summary.
â€¦ (draft, patch, save) â€¦

user  â€º /dev off
assistant â€º Switched to codeâ€‘dev profile.

user  â€º summarise_logs
assistant â€º â€¢ Total requests:Â 1245
           â€¢ Errors:Â 17 (1.3â€¯%)
           â€¢ Slowest endpoint: /search (p95Â =Â 812â€¯ms)
```

---

## TestingÂ withÂ Scenarios

```bash
python automated_cli_client.py tests/greetings.json
```

`automated_cli_client.py` feeds user turns from the JSON array and waits for each DAG to finish (looks for `âœ“ Node n0` â†’ `task.done`). Perfect for CI.

---

## Contributing

1. Fork â†’ create a branch â†’ open a PR.
2. Target PythonÂ 3.11+, typeâ€‘annotate new code.
3. For new neuros: **lazyâ€‘import** external deps inside `run()` to avoid bloating the global environment.
4. Update `requirements.txt` only when absolutely necessary (serverâ€‘side libs, not perâ€‘neuro deps).

---

## License

MIT Â©Â 2025Â NeoÂ Contributors

> **OpenAI API** â€“ set `OPENAI_API_KEY` in your shell before starting the server.
